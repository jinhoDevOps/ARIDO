문서 2: 쿠버네티스 학습 과정
1. 쿠버네티스의 핵심 개념 
	- 포드(Pod), 노드(Node), 네임스페이스(Namespace) 등
	- 쿠버네티스의 네트워킹 및 스토리지 관리
2. 쿠버네티스 관리 및 운영
	- 쿠버네티스 클러스터 관리
	- 애플리케이션 배포 및 스케일링
	- 모니터링 및 로깅

---

# 1 쿠버네티스 동작원리

# 2 쿠버네티스 컴포넌트
![[Pasted image 20231226115053.png]]
마스터 컴포넌트:

- etcd - key-value 데이터의 저장소
- kube-apiserver - k8s API를 사용하는 모든 요청을 받고 요청에 유효한지 검사
- kube-scheduler - 파드를 실행할 노드 선택
- kube-controller-manager  - 파드를 관찰하며 개수를 보장

워커 노드 컴포넌트:

- kubelet - 모든 노드에서 실행되는 k8s 에이전트, 데몬 형태로 동작
- kube-proxy - k8s의 network 통신을 관리, iptables rule을 구성
- 컨테이너 런타임 - 컨테이너를 실행하는 엔진  , docker, containerd, runc

`etcd`는 쿠버네티스 클러스터의 핵심 컴포넌트로, 클러스터의 모든 상태 정보를 저장하는 분산 키-값 저장소입니다. 쿠버네티스 클러스터의 모든 구성과 상태 데이터(예: 파드, 서비스, 레플리카셋 등)를 etcd에 저장합니다. 이 데이터는 클러스터를 관리하고 조정하는 데 필수적입니다.

간단히 말해, etcd는 쿠버네티스 클러스터의 '두뇌'와 같은 역할을 합니다. 클러스터의 모든 변경 사항은 etcd에 저장되며, 쿠버네티스 API 서버는 etcd와 통신하여 이 정보를 조회하고 업데이트합니다. etcd의 주요 특징은 다음과 같습니다:

- **분산 시스템**: etcd는 고가용성을 보장하기 위해 여러 서버(노드)에 걸쳐 분산되어 있습니다.
- **데이터 일관성**: 데이터의 일관성을 유지하기 위해 강력한 일관성 모델을 사용합니다.
- **보안**: 클러스터의 중요한 정보를 담고 있기 때문에, 데이터 암호화와 접근 제어를 통해 보안을 강화합니다.
- **모니터링과 백업**: 클러스터의 안정성을 유지하기 위해 etcd의 상태를 지속적으로 모니터링하고 정기적으로 백업합니다.

간단히 말하면, etcd는 쿠버네티스 클러스터의 중앙 데이터 저장소로서, 클러스터의 모든 주요 상태 정보를 저장하고, 클러스터의 안정성과 일관성을 유지하는 데 중요한 역할을 합니다.

---
# 3 쿠버네티스 네임스페이스

- 네임스페이스

## 4 yaml 템플릿, api
사람 친화적인 데이터 직렬화 양식
탑다운 아니고 통으로 읽어서 쿠버api문법 맞는지 확인

api 버전은 kubectl explan 명령 쓰면 확인가능



# 5. 파드

# 6 레플리케이션 컨트롤러

2024-01-18
일단 듣는중


---
참고 문서
[따배쿠 시리즈](https://www.youtube.com/playlist?list=PLApuRlvrZKohaBHvXAOhUD-RxD0uQ3z0c)


6-2 레플리카 셋 
https://kubernetes.io/ko/docs/concepts/workloads/controllers/replicaset/

6-3 Deployment for RollingUpdate
2024-01-19
![[Pasted image 20240119111145.png]]


디플로이먼트가 상위항목임
deployment --> ReplicaSet --> Pod
- 레플리카셋이랑 디플로이먼트랑 거의 똑같음

---
2024-01-23
디플로이먼트 적용해보려고 kind 문서 보고 진행


```sh
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
```


2024-01-29
6-4. DaemonSet
데몬셋 - 롤링업데이트
- 전체 노드에서 파드가 한개씩 실행되도록 보장
- 로그 수집기, 모니터링 에이전트 같은 프로그램 실행시 적용
- 롤링업데이트 기능도 있음
- yaml 에 컨테이너 종류만 적어주면 노드당 파드하나 보장
	로그/모니터링 에이전트에 적합한 컨트롤러다


6-5. Stateful sets
스테이트풀셋
- 파드 이름
- 파드의 볼륨(스토리지)
이걸 유지시켜준다.
후에 serviceName 하고 파드 이름하고 연결해서 core dns 도메인 이름으로 사용가능
- 스케일 업/다운? 확인하면서 쓸수있고, 항상 이름 보장받음

6-6. job controller
24/7 runnig중인 파드 보장
https://kubernetes.io/ko/docs/concepts/workloads/controllers/job/

`kubectl run testpod --image=centos:7 --command sleep 5`
이런식으로 실행하면 5초뒤에 파드종료 -> 그럼 또 다시 쿠버가 살림 -> 계속 증가됨
![[Pasted image 20240129120337.png]]
![[Pasted image 20240129121042.png]]
심지어는 에러가 나도 마찬가지

백업 해줘!
잡(ex백업 어플리케이션)
잡컨트롤러
	파드1에 백업 성공-> 끝
	파드2에 백업 실패 -> 새로운 파드 생성
잡 성공여부에 따라 새로 생성할지말지 해주는게 잡 컨트롤러

예시 job정의 내용중 restartPolicy: 내용
spec.template.spec.restartPolicy = "OnFailure"이렇게 컨테이너 재시작 정책이 OnFailure로 설정되어 있다면 포드는 원래 실행중이던 노드에서 컨테이너를 재시작합니다. 그래서 컨테이너로 띄운 앱에서 이렇게 컨테이너가 재시작되는 경우를 감안해서 만들어져 있어야 합니다. 그렇게 되어 있지 않다면 재시작 옵션을 "Never"로 줘서 재시작되는걸 막아야 합니다. 
출처: https://arisu1000.tistory.com/27836 [아리수:티스토리]



6-7 CronJob
위의 job을 컨트롤 하는거임
cronjob 정의 하는것도 job과 동일한데
```
spec: 
  schedule: "0 3 1 * * "
  JobTemplate:
    spec:
    template: 
    ...
```
이런식으로 상위에 크론 스케쥴 추가 


=> 어쨌든 컨트롤러는 노드에 파드 내려주고, 파드 개수보장 역할인듯
	 그 방식이 조금씩 다를뿐

# 7 서비스
- 쿠버네티스 네트워크

여기서 어렵다고 느낌

컨트롤러로 webui 파드 3개 실행했어 > 동일한 어플 돌아가는 상황인데 어찌 접근함? > service 가 api 통해서 webui 파드들 하나로 묶어서 관리해라 > app: webui 라벨 가지고있는애들 묶어

2024-01-30

1. 컨트롤러(Deployment)로 app:webui 라벨을 가진 파드3개 생성
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webui-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webui
  template:
    metadata:
      labels:
        app: webui
    spec:
      containers:
      - name: webui
        image: nginx  # 예시로 nginx 이미지 사용
        ports:
        - containerPort: 80
```
2. 이걸 쿠버 클러스터에 적용
	`kubectl apply -f webui-deployment.yaml`
	헉 그럼 3개인데 어찌 접근함? -> 이파드들의 접근 관리하는 서비스 생성
3. 서비스 생성 `webui-service.yaml` 예시파일:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webui-service
spec:
  selector:
    app: webui
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
```
이 서비스는 `app: webui` 라벨을 가진 모든 파드에 대한 트래픽을 받아들이고, 파드의 `80`번 포트로 전달합니다. `ClusterIP` 타입은 서비스에 클러스터 내부 IP를 할당하여 클러스터 내에서만 접근할 수 있게 합니다.

4. 서비스 적용
	1. `kubectl apply -f webui-service.yaml`
	2. `webui-service`는 `app: webui` 라벨을 가진 3개의 파드로 들어오는 트래픽을 로드 밸런싱합니다. 클라이언트는 `webui-service`의 IP 주소를 사용하여 이 파드들에 접근할 수 있으며, 서비스는 자동으로 트래픽을 파드들 사이에 분산시킵니다.
	3. 이 방식을 통해, 개별 파드의 IP 주소를 알 필요 없이, 일관된 방법으로 애플리케이션에 접근할 수 있게 됩니다. 서비스는 파드가 다운되거나 새로 생성될 때에도 동일한 접근 방식을 유지해줍니다.

파드 ip 묶어서 dns 부여하고 lb 수행함으로서 하나로 묶어서 서비스로 노출하는 애

서비스 타입
ClusterIP, NodePort, LoadBalancer(클라우드플랫폼에서만 됨), ExternalName
clusterIP가 기본이고 여기에 기능 추가한게 후술한애들

api service는 뭐다? 파드(어플리케이션)단일 진입점을 virual IP로 제공한다.

selector의 label이 동일한 파드들의 그룹으로 묶음
단일 진입점(virtual_IP)
클러스터 내부에서만 사용가능 << ?

- ClusterIP
	cluster IP 가 단일진입점이고 virtual_IP네

- NodePort
	클러스터ip + 노드포트 범위 할당 (외부에서 접속 가능한 포트 예약)


- LoadBalancer(클라우드플랫폼에서만 됨)

- ExternalName

